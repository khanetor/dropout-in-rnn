{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rnn import StochasticLSTM\n",
    "from utils import DropoutBCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_table = pd.read_table(\"./data/hill-valley/Hill_Valley_without_noise_Training.data\", sep=',', dtype=np.float64)\n",
    "testing_table = pd.read_table(\"./data/hill-valley/Hill_Valley_without_noise_Testing.data\", sep=',', dtype=np.float64)\n",
    "\n",
    "input_columns = [f\"X{i}\" for i in range(1, 101)]\n",
    "label_column = \"class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X92</th>\n",
       "      <th>X93</th>\n",
       "      <th>X94</th>\n",
       "      <th>X95</th>\n",
       "      <th>X96</th>\n",
       "      <th>X97</th>\n",
       "      <th>X98</th>\n",
       "      <th>X99</th>\n",
       "      <th>X100</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1317.265789</td>\n",
       "      <td>1315.220951</td>\n",
       "      <td>1312.770581</td>\n",
       "      <td>1309.834252</td>\n",
       "      <td>1306.315588</td>\n",
       "      <td>1302.099102</td>\n",
       "      <td>1297.046401</td>\n",
       "      <td>1290.991646</td>\n",
       "      <td>1283.736109</td>\n",
       "      <td>1275.041652</td>\n",
       "      <td>...</td>\n",
       "      <td>1327.575109</td>\n",
       "      <td>1327.575350</td>\n",
       "      <td>1327.575552</td>\n",
       "      <td>1327.575719</td>\n",
       "      <td>1327.575859</td>\n",
       "      <td>1327.575976</td>\n",
       "      <td>1327.576074</td>\n",
       "      <td>1327.576155</td>\n",
       "      <td>1327.576223</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7329.967624</td>\n",
       "      <td>7379.907443</td>\n",
       "      <td>7441.799231</td>\n",
       "      <td>7518.503422</td>\n",
       "      <td>7613.565031</td>\n",
       "      <td>7731.377492</td>\n",
       "      <td>7877.385707</td>\n",
       "      <td>8058.337694</td>\n",
       "      <td>8282.596458</td>\n",
       "      <td>8560.526497</td>\n",
       "      <td>...</td>\n",
       "      <td>7121.300474</td>\n",
       "      <td>7121.300438</td>\n",
       "      <td>7121.300410</td>\n",
       "      <td>7121.300387</td>\n",
       "      <td>7121.300368</td>\n",
       "      <td>7121.300353</td>\n",
       "      <td>7121.300341</td>\n",
       "      <td>7121.300331</td>\n",
       "      <td>7121.300323</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>809.421410</td>\n",
       "      <td>809.780119</td>\n",
       "      <td>810.207191</td>\n",
       "      <td>810.715653</td>\n",
       "      <td>811.321016</td>\n",
       "      <td>812.041748</td>\n",
       "      <td>812.899834</td>\n",
       "      <td>813.921452</td>\n",
       "      <td>815.137768</td>\n",
       "      <td>816.585886</td>\n",
       "      <td>...</td>\n",
       "      <td>807.545134</td>\n",
       "      <td>807.544181</td>\n",
       "      <td>807.543381</td>\n",
       "      <td>807.542709</td>\n",
       "      <td>807.542144</td>\n",
       "      <td>807.541670</td>\n",
       "      <td>807.541272</td>\n",
       "      <td>807.540937</td>\n",
       "      <td>807.540656</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45334.208880</td>\n",
       "      <td>45334.213560</td>\n",
       "      <td>45334.219060</td>\n",
       "      <td>45334.225500</td>\n",
       "      <td>45334.233050</td>\n",
       "      <td>45334.241910</td>\n",
       "      <td>45334.252300</td>\n",
       "      <td>45334.264480</td>\n",
       "      <td>45334.278760</td>\n",
       "      <td>45334.295520</td>\n",
       "      <td>...</td>\n",
       "      <td>47550.921710</td>\n",
       "      <td>47224.457710</td>\n",
       "      <td>46946.072760</td>\n",
       "      <td>46708.686150</td>\n",
       "      <td>46506.259970</td>\n",
       "      <td>46333.645520</td>\n",
       "      <td>46186.452370</td>\n",
       "      <td>46060.936670</td>\n",
       "      <td>45953.905930</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.810359</td>\n",
       "      <td>1.810359</td>\n",
       "      <td>1.810359</td>\n",
       "      <td>1.810359</td>\n",
       "      <td>1.810359</td>\n",
       "      <td>1.810359</td>\n",
       "      <td>1.810359</td>\n",
       "      <td>1.810359</td>\n",
       "      <td>1.810359</td>\n",
       "      <td>1.810359</td>\n",
       "      <td>...</td>\n",
       "      <td>1.790275</td>\n",
       "      <td>1.794794</td>\n",
       "      <td>1.798296</td>\n",
       "      <td>1.801010</td>\n",
       "      <td>1.803114</td>\n",
       "      <td>1.804744</td>\n",
       "      <td>1.806008</td>\n",
       "      <td>1.806987</td>\n",
       "      <td>1.807746</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             X1            X2            X3            X4            X5  \\\n",
       "0   1317.265789   1315.220951   1312.770581   1309.834252   1306.315588   \n",
       "1   7329.967624   7379.907443   7441.799231   7518.503422   7613.565031   \n",
       "2    809.421410    809.780119    810.207191    810.715653    811.321016   \n",
       "3  45334.208880  45334.213560  45334.219060  45334.225500  45334.233050   \n",
       "4      1.810359      1.810359      1.810359      1.810359      1.810359   \n",
       "\n",
       "             X6            X7            X8            X9           X10  ...  \\\n",
       "0   1302.099102   1297.046401   1290.991646   1283.736109   1275.041652  ...   \n",
       "1   7731.377492   7877.385707   8058.337694   8282.596458   8560.526497  ...   \n",
       "2    812.041748    812.899834    813.921452    815.137768    816.585886  ...   \n",
       "3  45334.241910  45334.252300  45334.264480  45334.278760  45334.295520  ...   \n",
       "4      1.810359      1.810359      1.810359      1.810359      1.810359  ...   \n",
       "\n",
       "            X92           X93           X94           X95           X96  \\\n",
       "0   1327.575109   1327.575350   1327.575552   1327.575719   1327.575859   \n",
       "1   7121.300474   7121.300438   7121.300410   7121.300387   7121.300368   \n",
       "2    807.545134    807.544181    807.543381    807.542709    807.542144   \n",
       "3  47550.921710  47224.457710  46946.072760  46708.686150  46506.259970   \n",
       "4      1.790275      1.794794      1.798296      1.801010      1.803114   \n",
       "\n",
       "            X97           X98           X99          X100  class  \n",
       "0   1327.575976   1327.576074   1327.576155   1327.576223    0.0  \n",
       "1   7121.300353   7121.300341   7121.300331   7121.300323    1.0  \n",
       "2    807.541670    807.541272    807.540937    807.540656    1.0  \n",
       "3  46333.645520  46186.452370  46060.936670  45953.905930    1.0  \n",
       "4      1.804744      1.806008      1.806987      1.807746    0.0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "dim = 1\n",
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(training_table[input_columns])\n",
    "\n",
    "x_train = scaler.transform(training_table[input_columns])\n",
    "y_train = training_table[label_column].values\n",
    "x_test = scaler.transform(testing_table[input_columns])\n",
    "y_test = testing_table[label_column].values\n",
    "\n",
    "N = len(y_train)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "        [(x_train[i], y_train[i]) for i in range(len(y_train))],\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=True\n",
    ")\n",
    "\n",
    "def format_input(input_batch):\n",
    "    return input_batch.transpose(1, 0).reshape(seq_length, -1, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = StochasticLSTM(1, 50, 0.75)\n",
    "        self.fc1 = nn.Linear(50, 25)\n",
    "        self.fc2 = nn.Linear(25, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[-1]\n",
    "        out = self.fc1(out)\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = DropoutBCELoss(1, 1, 0.2)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.703095\n",
      "[1,   100] loss: 0.698646\n",
      "[1,   150] loss: 0.699577\n",
      "[1,   200] loss: 0.700538\n",
      "[1,   250] loss: 0.699553\n",
      "[1,   300] loss: 0.710885\n",
      "[1,   350] loss: 0.698429\n",
      "[1,   400] loss: 0.700032\n",
      "[1,   450] loss: 0.710893\n",
      "[1,   500] loss: 0.700282\n",
      "[1,   550] loss: 0.702702\n",
      "[1,   600] loss: 0.702067\n",
      "[2,    50] loss: 0.699280\n",
      "[2,   100] loss: 0.701413\n",
      "[2,   150] loss: 0.701893\n",
      "[2,   200] loss: 0.703210\n",
      "[2,   250] loss: 0.704761\n",
      "[2,   300] loss: 0.702725\n",
      "[2,   350] loss: 0.700574\n",
      "[2,   400] loss: 0.700950\n",
      "[2,   450] loss: 0.704881\n",
      "[2,   500] loss: 0.700955\n",
      "[2,   550] loss: 0.702060\n",
      "[2,   600] loss: 0.700629\n",
      "[3,    50] loss: 0.703008\n",
      "[3,   100] loss: 0.702231\n",
      "[3,   150] loss: 0.701620\n",
      "[3,   200] loss: 0.702789\n",
      "[3,   250] loss: 0.702143\n",
      "[3,   300] loss: 0.701335\n",
      "[3,   350] loss: 0.698815\n",
      "[3,   400] loss: 0.704552\n",
      "[3,   450] loss: 0.699986\n",
      "[3,   500] loss: 0.703981\n",
      "[3,   550] loss: 0.701397\n",
      "[3,   600] loss: 0.702366\n",
      "[4,    50] loss: 0.701985\n",
      "[4,   100] loss: 0.700976\n",
      "[4,   150] loss: 0.700593\n",
      "[4,   200] loss: 0.702287\n",
      "[4,   250] loss: 0.697395\n",
      "[4,   300] loss: 0.705158\n",
      "[4,   350] loss: 0.701370\n",
      "[4,   400] loss: 0.702492\n",
      "[4,   450] loss: 0.702360\n",
      "[4,   500] loss: 0.702246\n",
      "[4,   550] loss: 0.698487\n",
      "[4,   600] loss: 0.701795\n",
      "Finish training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_dl):\n",
    "        inputs = format_input(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs.flatten(), labels.double(), net.rnn.named_parameters(), N)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.6f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finish training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = net(torch.tensor(x_test.reshape(606, 100, 1).swapaxes(1, 0)))\n",
    "    outputs = outputs.flatten().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.508485476047741"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
