{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rnn import StochasticLSTM\n",
    "from utils import weight_coefficient, bias_coefficient, filter_parameters, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_table = pd.read_table(\"./data/hill-valley/Hill_Valley_with_noise_Training.data\", sep=',', dtype=np.float64)\n",
    "testing_table = pd.read_table(\"./data/hill-valley/Hill_Valley_with_noise_Testing.data\", sep=',', dtype=np.float64)\n",
    "\n",
    "input_columns = [f\"X{i}\" for i in range(1, 101)]\n",
    "label_column = \"class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X92</th>\n",
       "      <th>X93</th>\n",
       "      <th>X94</th>\n",
       "      <th>X95</th>\n",
       "      <th>X96</th>\n",
       "      <th>X97</th>\n",
       "      <th>X98</th>\n",
       "      <th>X99</th>\n",
       "      <th>X100</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.02</td>\n",
       "      <td>36.49</td>\n",
       "      <td>38.20</td>\n",
       "      <td>38.85</td>\n",
       "      <td>39.38</td>\n",
       "      <td>39.74</td>\n",
       "      <td>37.02</td>\n",
       "      <td>39.53</td>\n",
       "      <td>38.81</td>\n",
       "      <td>38.79</td>\n",
       "      <td>...</td>\n",
       "      <td>36.62</td>\n",
       "      <td>36.92</td>\n",
       "      <td>38.80</td>\n",
       "      <td>38.52</td>\n",
       "      <td>38.07</td>\n",
       "      <td>36.73</td>\n",
       "      <td>39.46</td>\n",
       "      <td>37.50</td>\n",
       "      <td>39.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.83</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.78</td>\n",
       "      <td>...</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68177.69</td>\n",
       "      <td>66138.42</td>\n",
       "      <td>72981.88</td>\n",
       "      <td>74304.33</td>\n",
       "      <td>67549.66</td>\n",
       "      <td>69367.34</td>\n",
       "      <td>69169.41</td>\n",
       "      <td>73268.61</td>\n",
       "      <td>74465.84</td>\n",
       "      <td>72503.37</td>\n",
       "      <td>...</td>\n",
       "      <td>73438.88</td>\n",
       "      <td>71053.35</td>\n",
       "      <td>71112.62</td>\n",
       "      <td>74916.48</td>\n",
       "      <td>72571.58</td>\n",
       "      <td>66348.97</td>\n",
       "      <td>71063.72</td>\n",
       "      <td>67404.27</td>\n",
       "      <td>74920.24</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44889.06</td>\n",
       "      <td>39191.86</td>\n",
       "      <td>40728.46</td>\n",
       "      <td>38576.36</td>\n",
       "      <td>45876.06</td>\n",
       "      <td>47034.00</td>\n",
       "      <td>46611.43</td>\n",
       "      <td>37668.32</td>\n",
       "      <td>40980.89</td>\n",
       "      <td>38466.15</td>\n",
       "      <td>...</td>\n",
       "      <td>42625.67</td>\n",
       "      <td>40684.20</td>\n",
       "      <td>46960.73</td>\n",
       "      <td>44546.80</td>\n",
       "      <td>45410.53</td>\n",
       "      <td>47139.44</td>\n",
       "      <td>43095.68</td>\n",
       "      <td>40888.34</td>\n",
       "      <td>39615.19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.70</td>\n",
       "      <td>5.40</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.38</td>\n",
       "      <td>5.27</td>\n",
       "      <td>5.61</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.38</td>\n",
       "      <td>5.34</td>\n",
       "      <td>5.87</td>\n",
       "      <td>...</td>\n",
       "      <td>5.17</td>\n",
       "      <td>5.67</td>\n",
       "      <td>5.60</td>\n",
       "      <td>5.94</td>\n",
       "      <td>5.73</td>\n",
       "      <td>5.22</td>\n",
       "      <td>5.30</td>\n",
       "      <td>5.73</td>\n",
       "      <td>5.91</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0     39.02     36.49     38.20     38.85     39.38     39.74     37.02   \n",
       "1      1.83      1.71      1.77      1.77      1.68      1.78      1.80   \n",
       "2  68177.69  66138.42  72981.88  74304.33  67549.66  69367.34  69169.41   \n",
       "3  44889.06  39191.86  40728.46  38576.36  45876.06  47034.00  46611.43   \n",
       "4      5.70      5.40      5.28      5.38      5.27      5.61      6.00   \n",
       "\n",
       "         X8        X9       X10  ...       X92       X93       X94       X95  \\\n",
       "0     39.53     38.81     38.79  ...     36.62     36.92     38.80     38.52   \n",
       "1      1.70      1.75      1.78  ...      1.80      1.79      1.77      1.74   \n",
       "2  73268.61  74465.84  72503.37  ...  73438.88  71053.35  71112.62  74916.48   \n",
       "3  37668.32  40980.89  38466.15  ...  42625.67  40684.20  46960.73  44546.80   \n",
       "4      5.38      5.34      5.87  ...      5.17      5.67      5.60      5.94   \n",
       "\n",
       "        X96       X97       X98       X99      X100  class  \n",
       "0     38.07     36.73     39.46     37.50     39.10    0.0  \n",
       "1      1.74      1.80      1.78      1.75      1.69    1.0  \n",
       "2  72571.58  66348.97  71063.72  67404.27  74920.24    1.0  \n",
       "3  45410.53  47139.44  43095.68  40888.34  39615.19    0.0  \n",
       "4      5.73      5.22      5.30      5.73      5.91    0.0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 1\n",
    "seq_length = 100\n",
    "DROP = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X91</th>\n",
       "      <th>X92</th>\n",
       "      <th>X93</th>\n",
       "      <th>X94</th>\n",
       "      <th>X95</th>\n",
       "      <th>X96</th>\n",
       "      <th>X97</th>\n",
       "      <th>X98</th>\n",
       "      <th>X99</th>\n",
       "      <th>X100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.02</td>\n",
       "      <td>36.49</td>\n",
       "      <td>38.20</td>\n",
       "      <td>38.85</td>\n",
       "      <td>39.38</td>\n",
       "      <td>39.74</td>\n",
       "      <td>37.02</td>\n",
       "      <td>39.53</td>\n",
       "      <td>38.81</td>\n",
       "      <td>38.79</td>\n",
       "      <td>...</td>\n",
       "      <td>37.57</td>\n",
       "      <td>36.62</td>\n",
       "      <td>36.92</td>\n",
       "      <td>38.80</td>\n",
       "      <td>38.52</td>\n",
       "      <td>38.07</td>\n",
       "      <td>36.73</td>\n",
       "      <td>39.46</td>\n",
       "      <td>37.50</td>\n",
       "      <td>39.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.83</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.78</td>\n",
       "      <td>...</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68177.69</td>\n",
       "      <td>66138.42</td>\n",
       "      <td>72981.88</td>\n",
       "      <td>74304.33</td>\n",
       "      <td>67549.66</td>\n",
       "      <td>69367.34</td>\n",
       "      <td>69169.41</td>\n",
       "      <td>73268.61</td>\n",
       "      <td>74465.84</td>\n",
       "      <td>72503.37</td>\n",
       "      <td>...</td>\n",
       "      <td>69384.71</td>\n",
       "      <td>73438.88</td>\n",
       "      <td>71053.35</td>\n",
       "      <td>71112.62</td>\n",
       "      <td>74916.48</td>\n",
       "      <td>72571.58</td>\n",
       "      <td>66348.97</td>\n",
       "      <td>71063.72</td>\n",
       "      <td>67404.27</td>\n",
       "      <td>74920.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44889.06</td>\n",
       "      <td>39191.86</td>\n",
       "      <td>40728.46</td>\n",
       "      <td>38576.36</td>\n",
       "      <td>45876.06</td>\n",
       "      <td>47034.00</td>\n",
       "      <td>46611.43</td>\n",
       "      <td>37668.32</td>\n",
       "      <td>40980.89</td>\n",
       "      <td>38466.15</td>\n",
       "      <td>...</td>\n",
       "      <td>47653.60</td>\n",
       "      <td>42625.67</td>\n",
       "      <td>40684.20</td>\n",
       "      <td>46960.73</td>\n",
       "      <td>44546.80</td>\n",
       "      <td>45410.53</td>\n",
       "      <td>47139.44</td>\n",
       "      <td>43095.68</td>\n",
       "      <td>40888.34</td>\n",
       "      <td>39615.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.70</td>\n",
       "      <td>5.40</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.38</td>\n",
       "      <td>5.27</td>\n",
       "      <td>5.61</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.38</td>\n",
       "      <td>5.34</td>\n",
       "      <td>5.87</td>\n",
       "      <td>...</td>\n",
       "      <td>5.52</td>\n",
       "      <td>5.17</td>\n",
       "      <td>5.67</td>\n",
       "      <td>5.60</td>\n",
       "      <td>5.94</td>\n",
       "      <td>5.73</td>\n",
       "      <td>5.22</td>\n",
       "      <td>5.30</td>\n",
       "      <td>5.73</td>\n",
       "      <td>5.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>2064.63</td>\n",
       "      <td>2174.44</td>\n",
       "      <td>2249.02</td>\n",
       "      <td>2249.60</td>\n",
       "      <td>2307.61</td>\n",
       "      <td>2328.84</td>\n",
       "      <td>2071.90</td>\n",
       "      <td>2107.05</td>\n",
       "      <td>2242.68</td>\n",
       "      <td>2183.03</td>\n",
       "      <td>...</td>\n",
       "      <td>2325.98</td>\n",
       "      <td>2202.95</td>\n",
       "      <td>2248.56</td>\n",
       "      <td>2185.22</td>\n",
       "      <td>2145.38</td>\n",
       "      <td>2288.64</td>\n",
       "      <td>2288.42</td>\n",
       "      <td>2096.08</td>\n",
       "      <td>2114.25</td>\n",
       "      <td>2281.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>9.42</td>\n",
       "      <td>9.30</td>\n",
       "      <td>9.66</td>\n",
       "      <td>9.63</td>\n",
       "      <td>9.15</td>\n",
       "      <td>9.91</td>\n",
       "      <td>9.45</td>\n",
       "      <td>9.84</td>\n",
       "      <td>9.36</td>\n",
       "      <td>9.15</td>\n",
       "      <td>...</td>\n",
       "      <td>9.19</td>\n",
       "      <td>9.85</td>\n",
       "      <td>9.76</td>\n",
       "      <td>9.82</td>\n",
       "      <td>9.64</td>\n",
       "      <td>9.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.23</td>\n",
       "      <td>9.82</td>\n",
       "      <td>9.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>5.31</td>\n",
       "      <td>5.03</td>\n",
       "      <td>5.17</td>\n",
       "      <td>5.02</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.16</td>\n",
       "      <td>5.11</td>\n",
       "      <td>5.27</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>5.08</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.06</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.27</td>\n",
       "      <td>5.14</td>\n",
       "      <td>5.12</td>\n",
       "      <td>5.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>1.03</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.97</td>\n",
       "      <td>...</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>61.77</td>\n",
       "      <td>58.63</td>\n",
       "      <td>63.06</td>\n",
       "      <td>58.44</td>\n",
       "      <td>57.66</td>\n",
       "      <td>67.63</td>\n",
       "      <td>58.16</td>\n",
       "      <td>61.59</td>\n",
       "      <td>60.71</td>\n",
       "      <td>58.07</td>\n",
       "      <td>...</td>\n",
       "      <td>69.10</td>\n",
       "      <td>59.52</td>\n",
       "      <td>58.88</td>\n",
       "      <td>63.51</td>\n",
       "      <td>57.27</td>\n",
       "      <td>60.08</td>\n",
       "      <td>62.75</td>\n",
       "      <td>65.13</td>\n",
       "      <td>59.74</td>\n",
       "      <td>63.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>606 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0       39.02     36.49     38.20     38.85     39.38     39.74     37.02   \n",
       "1        1.83      1.71      1.77      1.77      1.68      1.78      1.80   \n",
       "2    68177.69  66138.42  72981.88  74304.33  67549.66  69367.34  69169.41   \n",
       "3    44889.06  39191.86  40728.46  38576.36  45876.06  47034.00  46611.43   \n",
       "4        5.70      5.40      5.28      5.38      5.27      5.61      6.00   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "601   2064.63   2174.44   2249.02   2249.60   2307.61   2328.84   2071.90   \n",
       "602      9.42      9.30      9.66      9.63      9.15      9.91      9.45   \n",
       "603      5.31      5.03      5.17      5.02      5.20      5.16      5.11   \n",
       "604      1.03      1.02      1.09      1.06      1.03      1.03      0.98   \n",
       "605     61.77     58.63     63.06     58.44     57.66     67.63     58.16   \n",
       "\n",
       "           X8        X9       X10  ...       X91       X92       X93  \\\n",
       "0       39.53     38.81     38.79  ...     37.57     36.62     36.92   \n",
       "1        1.70      1.75      1.78  ...      1.71      1.80      1.79   \n",
       "2    73268.61  74465.84  72503.37  ...  69384.71  73438.88  71053.35   \n",
       "3    37668.32  40980.89  38466.15  ...  47653.60  42625.67  40684.20   \n",
       "4        5.38      5.34      5.87  ...      5.52      5.17      5.67   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "601   2107.05   2242.68   2183.03  ...   2325.98   2202.95   2248.56   \n",
       "602      9.84      9.36      9.15  ...      9.19      9.85      9.76   \n",
       "603      5.27      5.19      5.13  ...      5.08      5.19      5.06   \n",
       "604      0.97      1.13      0.97  ...      1.11      1.11      1.06   \n",
       "605     61.59     60.71     58.07  ...     69.10     59.52     58.88   \n",
       "\n",
       "          X94       X95       X96       X97       X98       X99      X100  \n",
       "0       38.80     38.52     38.07     36.73     39.46     37.50     39.10  \n",
       "1        1.77      1.74      1.74      1.80      1.78      1.75      1.69  \n",
       "2    71112.62  74916.48  72571.58  66348.97  71063.72  67404.27  74920.24  \n",
       "3    46960.73  44546.80  45410.53  47139.44  43095.68  40888.34  39615.19  \n",
       "4        5.60      5.94      5.73      5.22      5.30      5.73      5.91  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "601   2185.22   2145.38   2288.64   2288.42   2096.08   2114.25   2281.91  \n",
       "602      9.82      9.64      9.40      9.53      9.23      9.82      9.57  \n",
       "603      5.28      5.28      5.19      5.27      5.14      5.12      5.04  \n",
       "604      0.96      1.06      1.01      1.07      1.10      1.11      1.07  \n",
       "605     63.51     57.27     60.08     62.75     65.13     59.74     63.86  \n",
       "\n",
       "[606 rows x 100 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_table.drop(\"class\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_inputs(x_inputs):\n",
    "    median = np.median(x_inputs, axis=1).reshape(-1, 1)\n",
    "    shifted_x = x_inputs - median\n",
    "    scale = np.abs(shifted_x).max(axis=1).reshape(-1, 1)\n",
    "    return shifted_x / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scale_inputs(training_table.drop(\"class\", axis=1).values)\n",
    "y_train = training_table[\"class\"].values\n",
    "x_test = scale_inputs(testing_table.drop(\"class\", axis=1).values)\n",
    "y_test = testing_table[\"class\"].values\n",
    "\n",
    "N = len(y_train)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "        [(x_train[i], y_train[i]) for i in range(len(y_train))],\n",
    "        batch_size=10,\n",
    "        num_workers=2,\n",
    "        shuffle=True\n",
    ")\n",
    "\n",
    "def format_input(input_batch):\n",
    "    return input_batch.transpose(1, 0).reshape(seq_length, -1, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = StochasticLSTM(1, 10, DROP)\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = torch.tanh(out[-1])\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().double()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "M = filter_parameters(net.named_parameters(), \"rnn\", \"weight\")\n",
    "m = filter_parameters(net.named_parameters(), \"rnn\", \"bias\")\n",
    "other_params = filter_parameters(net.named_parameters(), \"fc\", \"\")\n",
    "\n",
    "parameters = [\n",
    "    {\"params\": M, \"weight_decay\": weight_coefficient(1, 1, DROP, N)}, # dropout rnn weight\n",
    "    {\"params\": m, \"weight_decay\": bias_coefficient(1, 1, N)}, # dropout rnn bias\n",
    "    {\"params\": other_params} # other parameters\n",
    "]\n",
    "optimizer = optim.Adam(parameters, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    60] loss: 0.691331\n",
      "[2,    60] loss: 0.687976\n",
      "[3,    60] loss: 0.684752\n",
      "[4,    60] loss: 0.677401\n",
      "[5,    60] loss: 0.665819\n",
      "[6,    60] loss: 0.632883\n",
      "[7,    60] loss: 0.596804\n",
      "[8,    60] loss: 0.572507\n",
      "[9,    60] loss: 0.548868\n",
      "[10,    60] loss: 0.549237\n",
      "[11,    60] loss: 0.522626\n",
      "[12,    60] loss: 0.505411\n",
      "[13,    60] loss: 0.478787\n",
      "[14,    60] loss: 0.479767\n",
      "[15,    60] loss: 0.474467\n",
      "[16,    60] loss: 0.451829\n",
      "[17,    60] loss: 0.469593\n",
      "[18,    60] loss: 0.466033\n",
      "[19,    60] loss: 0.462533\n",
      "[20,    60] loss: 0.456150\n",
      "[21,    60] loss: 0.450837\n",
      "[22,    60] loss: 0.439042\n",
      "[23,    60] loss: 0.446072\n",
      "[24,    60] loss: 0.448176\n",
      "[25,    60] loss: 0.453790\n",
      "[26,    60] loss: 0.425476\n",
      "[27,    60] loss: 0.430693\n",
      "[28,    60] loss: 0.429815\n",
      "[29,    60] loss: 0.422505\n",
      "[30,    60] loss: 0.430798\n",
      "[31,    60] loss: 0.428890\n",
      "[32,    60] loss: 0.412452\n",
      "[33,    60] loss: 0.407932\n",
      "[34,    60] loss: 0.411501\n",
      "[35,    60] loss: 0.413657\n",
      "[36,    60] loss: 0.398388\n",
      "[37,    60] loss: 0.400976\n",
      "[38,    60] loss: 0.391840\n",
      "[39,    60] loss: 0.400744\n",
      "[40,    60] loss: 0.401815\n",
      "[41,    60] loss: 0.392431\n",
      "[42,    60] loss: 0.378702\n",
      "[43,    60] loss: 0.388266\n",
      "[44,    60] loss: 0.367563\n",
      "[45,    60] loss: 0.371319\n",
      "[46,    60] loss: 0.377810\n",
      "[47,    60] loss: 0.408820\n",
      "[48,    60] loss: 0.376262\n",
      "[49,    60] loss: 0.379518\n",
      "[50,    60] loss: 0.363613\n",
      "[51,    60] loss: 0.386640\n",
      "[52,    60] loss: 0.387681\n",
      "[53,    60] loss: 0.359462\n",
      "[54,    60] loss: 0.378551\n",
      "[55,    60] loss: 0.349885\n",
      "[56,    60] loss: 0.377146\n",
      "[57,    60] loss: 0.399816\n",
      "[58,    60] loss: 0.382036\n",
      "[59,    60] loss: 0.372552\n",
      "[60,    60] loss: 0.363608\n",
      "[61,    60] loss: 0.354242\n",
      "[62,    60] loss: 0.360728\n",
      "[63,    60] loss: 0.352146\n",
      "[64,    60] loss: 0.347602\n",
      "[65,    60] loss: 0.351169\n",
      "[66,    60] loss: 0.357608\n",
      "[67,    60] loss: 0.346952\n",
      "[68,    60] loss: 0.336752\n",
      "[69,    60] loss: 0.356636\n",
      "[70,    60] loss: 0.347250\n",
      "Finish training\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "for epoch in range(70):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_dl):\n",
    "        inputs = format_input(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs.flatten(), labels.double())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 60 == 59:\n",
    "            print('[%d, %5d] loss: %.6f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 60))\n",
    "            running_loss = 0.0\n",
    "\n",
    "net.eval()\n",
    "print(\"Finish training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = net(torch.tensor(x_test.reshape(606, 100, 1).swapaxes(1, 0)))\n",
    "    outputs = outputs.flatten().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9275434946019849"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
